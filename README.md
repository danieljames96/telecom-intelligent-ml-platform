# ğŸ“¡ Telecom ML Intelligence Platform

An end-to-end machine learning platform for telecom analytics featuring:
- ğŸ“‰ **Churn Prediction**
- âš ï¸ **Anomaly Detection**
- ğŸ”® **Traffic Forecasting**
- ğŸ“Š **Interactive Streamlit Dashboard** for visualization and insights

---

## ğŸ§  Project Highlights

- **Synthetic telecom dataset generation** including CDRs, user profiles, network behavior logs, and plan history
- **PySpark preprocessing** simulated within a Dockerized Jupyter environment
- **EDA** using Pandas, Matplotlib, and Seaborn
- **Model training** for:
  - Churn prediction using Random Forest + SMOTE
  - Network anomaly detection via Isolation Forest
  - Hourly traffic forecasting using Prophet
- **Modular code** in `src/` for clean ML pipelines, utilities, and inference
- **Streamlit dashboard** to interactively explore users, towers, traffic trends, and ML predictions

---

## ğŸ—‚ï¸ Directory Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                     # Generated raw synthetic datasets
â”‚   â””â”€â”€ processed/               # Cleaned Parquet datasets
â”œâ”€â”€ models/
â”‚   â””â”€â”€ *.pkl                    # Trained model files
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01_process_raw_to_processed.ipynb
â”‚   â””â”€â”€ 02_create_features.ipynb
â”‚   â””â”€â”€ 03_eda_analysis.ipynb
â”‚   â””â”€â”€ 04_model_churn_prediction.ipynb
â”‚   â””â”€â”€ 05_model_anomaly_detection.ipynb
â”‚   â””â”€â”€ 06_model_forecasting.ipynb
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_data.py         # Data generation logic
â”‚   â””â”€â”€ process_all_data.py      # Data processing logic
â””â”€â”€ src/
    â”œâ”€â”€ dashboard_app.py         # Streamlit dashboard
    â”œâ”€â”€ churn_model_utils.py         
    â”œâ”€â”€ anomaly_model_utils.py       
    â””â”€â”€ forecasting_model_utils.py   
```

---

## ğŸ—ƒï¸ Data Sources

All datasets used in this project are **synthetically generated** to simulate a realistic telecom environment:

- `synthetic_cdr.csv`: Call Detail Records with timestamps, tower usage, data and call volume
- `user_profiles.csv`: User demographics like age, region, and enterprise flags
- `user_plan_history.csv`: Monthly data usage and satisfaction scores
- `network_behaviors.csv`: Tower-level performance logs (latency, CPU, packet loss)
- `tower_locations.csv`: Geolocation and region info for each tower
- `ibm_telco_churn.csv`: Kaggle Dataset [IBM Telco Churn](https://www.kaggle.com/datasets/yeanzc/telco-customer-churn-ibm-dataset)

These datasets are joined, aggregated, and saved as Parquet using PySpark within a **Docker-based environment** to simulate distributed processing.

---

## ğŸš€ Getting Started

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Generate Data
```bash
python scripts/generate_data.py
```

### 3. Process Data (Docker is optional if pyspark not supported)
```bash
docker compose build
docker compose up
docker-compose exec pyspark-notebook python work/scripts/process_all_data.py
```

### 4. Launch Dashboard
```bash
streamlit run src/dashboard_app.py
```

---

## ğŸ“Œ Use Cases Covered

- Identify high-risk users based on churn probabilities
- Detect underperforming or anomalous towers
- Forecast future call volume to support capacity planning

---

## ğŸ Next Steps

- [ ] Add CSV upload support for new user churn predictions
- [ ] Add map visualizations for tower anomalies
- [ ] Add model monitoring features (e.g., prediction drift detection)

---

**Author:** Daniel James